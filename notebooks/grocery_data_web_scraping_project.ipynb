{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Import and install required libraries"
      ],
      "metadata": {
        "id": "YmELsJ-9dH44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_5LQ5mcPLo9",
        "outputId": "66963ccb-c46c-47b8-d39a-e20f989c9c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [1 InRelease 129 kB/129 kB 100%] [Connected to cloud.r-project.org (65.9.86.\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,189 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,556 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,393 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [929 kB]\n",
            "Fetched 7,330 kB in 3s (2,692 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "--2024-06-25 17:44:52--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 185.125.190.82, 185.125.190.81, 91.189.91.83, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|185.125.190.82|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-25 17:44:52 (262 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb’ saved [3708/3708]\n",
            "\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2024-06-25 17:44:53--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.128.190, 74.125.128.136, 74.125.128.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.128.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 108773084 (104M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 103.73M   231MB/s    in 0.4s    \n",
            "\n",
            "2024-06-25 17:44:54 (231 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [108773084/108773084]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 121929 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (126.0.6478.126-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2024-06-25 17:45:10--  https://chromedriver.storage.googleapis.com/[114.0.5735.90]/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 142.250.153.207, 142.250.145.207, 74.125.128.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|142.250.153.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-06-25 17:45:10 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open /tmp/chromedriver_linux64.zip, /tmp/chromedriver_linux64.zip.zip or /tmp/chromedriver_linux64.zip.ZIP.\n",
            "chmod: cannot access '/tmp/chromedriver': No such file or directory\n",
            "mv: cannot stat '/tmp/chromedriver': No such file or directory\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.25.1 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# Import the required libraries\n",
        "import sys\n",
        "import requests\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "# Set up the environment by executing the setup script\n",
        "# Note: This script installs necessary dependencies for Chrome WebDriver\n",
        "!sudo apt -y update\n",
        "!sudo apt install -y wget curl unzip\n",
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "!dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION = !curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE\n",
        "!wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "!unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "!chmod +x /tmp/chromedriver\n",
        "!mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
        "!pip install selenium\n",
        "!pip install python-dotenv\n",
        "\n",
        "# Additional libraries for web scraping setup\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, ElementNotInteractableException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver import ActionChains\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Chrome Options\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "D-pLyO4OdYSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')  # Run in headless mode\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "chrome_options.add_argument('--disable-gpu')\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\")\n",
        "chrome_options.add_argument(\"accept='text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\")\n",
        "chrome_options.add_argument(\"accept-encoding='gzip, deflate, br'\")\n",
        "chrome_options.add_argument(\"accept-language='en-US,en;q=0.5'\")\n",
        "\n",
        "# Initialize WebDriver instance\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "970bZVl5dcDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define utility functions\n",
        "1.   *take_screenshot*: Takes a screen shot of the web page in case of any errors\n",
        "2.   *handle_cookie_consent*: Handles cookie consent banners when opening up a product page\n",
        "3.    *detect_delimiter*: Used to identify the correct delimiter for csv files\n",
        "4.    *setup_google_drive*: Creates a save path for file storage in Google drive\n",
        "\n"
      ],
      "metadata": {
        "id": "rQtwI0fCdiwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define take_screenshot function.\n",
        "\n",
        "Main Functionality: Takes a screenshot of the current browser window.\n",
        "\"\"\"\n",
        "def take_screenshot(driver, product_link, debug=False):\n",
        "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    screenshot_name = f\"screenshot_{product_link}_{timestamp}.png\"\n",
        "    screenshot_path = os.path.join(os.getcwd(), screenshot_name)\n",
        "    driver.save_screenshot(screenshot_path)\n",
        "\n",
        "    # Debug print statement to confirm where the screenshot is saved\n",
        "    if debug:\n",
        "      print(f\"Screenshot saved: {screenshot_path}\")\n",
        "\n",
        "\"\"\"\n",
        "Define handle_cookie_consent function.\n",
        "\n",
        "Main Functionality: Handles cookie consent banners that may appear on web pages.\n",
        "\"\"\"\n",
        "def handle_cookie_consent(driver, debug=False):\n",
        "    try:\n",
        "        reject_button = WebDriverWait(driver, 10).until(\n",
        "            EC.element_to_be_clickable((By.ID, \"onetrust-reject-all-handler\"))\n",
        "        )\n",
        "\n",
        "        driver.execute_script(\"arguments[0].click();\", reject_button)\n",
        "\n",
        "        # Debug print statement to confirm cookie consent has been rejected\n",
        "        if debug:\n",
        "          print(\"Cookie consent rejected.\")\n",
        "\n",
        "        # Ensure the cookie consent banner is no longer visible\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.invisibility_of_element_located((By.ID, \"onetrust-reject-all-handler\"))\n",
        "        )\n",
        "\n",
        "        # Debug print statement to confirm the cookie consent banner is invisible after being rejected\n",
        "        if debug:\n",
        "          print(\"Cookie consent banner is now invisible.\")\n",
        "\n",
        "    except TimeoutException:\n",
        "      # Debug print statement to indicate no cookie consent banner was found\n",
        "      if debug:\n",
        "          print(\"No cookie consent banner found.\")\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    except WebDriverException as e:\n",
        "      # Debug print statement to log WebDriverException details\n",
        "      if debug:\n",
        "        print(f\"WebDriverException while handling cookie consent: {e}\")\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "\"\"\"\n",
        "Define detect_delimiter function.\n",
        "\n",
        "Main Functionality: Detect the delimiter of a CSV file. Detects the delimiter of a CSV file by analyzing the first few lines.\n",
        "                    If detection fails, falls back to a list of common delimiters and selects the best one.\n",
        "\"\"\"\n",
        "def detect_delimiter(file_path, default_delimiters=[',', ';', '\\t', '|']):\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        sample = file.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "\n",
        "        # Try to detect delimiter\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample)\n",
        "            delimiter = dialect.delimiter\n",
        "        except csv.Error:\n",
        "            # If the sniffer fails, try the default delimiters\n",
        "            delimiter_counts = {delim: sample.count(delim) for delim in default_delimiters}\n",
        "            # Select the delimiter with the highest count\n",
        "            delimiter = max(delimiter_counts, key=delimiter_counts.get)\n",
        "\n",
        "    return delimiter\n",
        "\n",
        "\"\"\"\n",
        "Define setup_google_drive function.\n",
        "\n",
        "Main Functionality: Mounts Google Drive and ensures the specified save path exists for file storage and retrieval.\n",
        "\"\"\"\n",
        "# Function to mount Google Drive and ensure save path exists\n",
        "def setup_google_drive(save_path='/content/drive/My Drive/scraped_files/', debug=False):\n",
        "    # Attempt to mount Google Drive\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "          print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    if debug:\n",
        "      print(f\"Save path {save_path} is ready.\")\n",
        "\n",
        "# Call the function to setup Google Drive\n",
        "setup_google_drive()"
      ],
      "metadata": {
        "id": "6POiOJ1adlxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f10133-cae5-4161-da0c-e30acbf4cdb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define main functions for scraping product data\n",
        "Functions responsible for the main scraping process of product data suc as product name, price, manufacturer etc:\n",
        "1.   *get_category_links*: Retrieves main category links from the specified page.\n",
        "2.   *scrape_visible_products*: Retrieves currently visible product containers on the page.\n",
        "3.   *scrape_product_data_from_containers*: Extracts product details from product containers.\n",
        "4.   *scrape_all_categories*: Main function that orchestrates scraping of product data from multiple categories.\n",
        "5.   *scrape_category_products*: Opens a category page, retrieves total number of products, and scrapes product data until all products are retrieved.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PbTqKaaUdxyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define get_category_links function.\n",
        "\n",
        "Main Functionality: Retrieves main category links from a given main page URL.\n",
        "\"\"\"\n",
        "def get_category_links(driver, main_page_url, max_retries=3, debug=False):\n",
        "    retry_count = 0\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            # Navigate to the main page and retrieve category links\n",
        "            driver.get(main_page_url)\n",
        "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'menu[data-testid=\"assortment-links\"]')))\n",
        "            assortment_menu = driver.find_element(By.CSS_SELECTOR, 'menu[data-testid=\"assortment-links\"]')\n",
        "            category_links = assortment_menu.find_elements(By.TAG_NAME, 'a')\n",
        "\n",
        "            # Get the main category link and navigate to it\n",
        "            main_category_links = [link.get_attribute('href') for link in category_links if not link.get_attribute('href').endswith('-')]\n",
        "\n",
        "            return main_category_links\n",
        "\n",
        "        except TimeoutException:\n",
        "            retry_count += 1\n",
        "\n",
        "            # Debug print statement to log if code failed to retrieve a specific category link\n",
        "            if debug:\n",
        "              print(f\"Timeout: Failed to retrieve category links. Retrying {retry_count}/{max_retries}...\")\n",
        "\n",
        "            # Add a delay between retries\n",
        "            time.sleep(5)\n",
        "\n",
        "        except NoSuchElementException:\n",
        "            retry_count += 1\n",
        "\n",
        "            # Debug print statement to log if a specific category link could not be found\n",
        "            if debug:\n",
        "              print(f\"Element not found: Assortment links menu. Retrying {retry_count}/{max_retries}...\")\n",
        "\n",
        "            # Add a delay between retries\n",
        "            time.sleep(5)\n",
        "\n",
        "        except WebDriverException as e:\n",
        "            retry_count += 1\n",
        "\n",
        "            # Debug print statement to log WebDriverException when trying to retrieve category link\n",
        "            if debug:\n",
        "              print(f\"WebDriverException during category link retrieval: {e}. Retrying {retry_count}/{max_retries}...\")\n",
        "\n",
        "            # Add a delay between retries\n",
        "            time.sleep(5)\n",
        "\n",
        "    # Debug print statement if code failed to retrieve category link after retries\n",
        "    if debug:\n",
        "      print(\"Failed to retrieve category links after multiple attempts.\")\n",
        "    return []\n",
        "\n",
        "\"\"\"\n",
        "Define scrape_visible_products function.\n",
        "\n",
        "Main Functionality: Retrieves currently visible product containers on a page.\n",
        "\"\"\"\n",
        "def scrape_visible_products(driver, debug=False):\n",
        "    try:\n",
        "        # Retrieve product containers that are currently visible\n",
        "        product_containers = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"product-container\"]')\n",
        "\n",
        "        return product_containers\n",
        "\n",
        "    except Exception as e:\n",
        "      if debug:\n",
        "        # Print debug error statement\n",
        "        print(f\"Error while scraping visible products: {e}\")\n",
        "\n",
        "      return []\n",
        "\n",
        "\"\"\"\n",
        "Define scrape_product_data_from_containers function.\n",
        "\n",
        "Main Functionality: Extracts product details from containers (e.g., product name, manufacturer, volume, prices).\n",
        "\"\"\"\n",
        "def scrape_product_data_from_containers(product_containers, category_link, viewed_product_links, debug=False):\n",
        "    product_data = []\n",
        "\n",
        "    for container in product_containers:\n",
        "        try:\n",
        "            # Get product link element\n",
        "            product_link_element = container.find_element(By.CSS_SELECTOR, 'p[data-testid=\"product-title\"] a')\n",
        "            product_link = product_link_element.get_attribute(\"href\")\n",
        "\n",
        "            if debug:\n",
        "              # Print the current product link being processed\n",
        "              print(\"product_link:\", product_link)\n",
        "\n",
        "            # Check if product link has been seen before\n",
        "            if product_link in viewed_product_links:\n",
        "                continue  # Skip this container if link has been seen\n",
        "\n",
        "            # Add product link to seen links set\n",
        "            viewed_product_links.add(product_link)\n",
        "\n",
        "            # Get product name element\n",
        "            product_name_element = container.find_element(By.CSS_SELECTOR, 'p[data-testid=\"product-title\"] a')\n",
        "            product_name = product_name_element.get_attribute(\"title\")\n",
        "\n",
        "            # Try to get manufacturer, set NaN if not found\n",
        "            try:\n",
        "                manufacturer_element = container.find_element(By.CSS_SELECTOR, 'span[data-testid=\"display-manufacturer\"]')\n",
        "                manufacturer_name = manufacturer_element.text.replace(\",\", \"\")\n",
        "            except NoSuchElementException:\n",
        "                manufacturer_name = np.nan  # Set to NaN if manufacturer is not found\n",
        "\n",
        "            # Try to get volume, set NaN if not found\n",
        "            try:\n",
        "                volume_element = container.find_element(By.CSS_SELECTOR, 'span[data-testid=\"display-volume\"]')\n",
        "                volume = volume_element.text\n",
        "            except NoSuchElementException:\n",
        "                volume = np.nan  # Set to NaN if volume is not found\n",
        "\n",
        "            # Try to get promotional compare price, then regular compare price, else NaN\n",
        "            try:\n",
        "                compare_price_element = container.find_element(By.CSS_SELECTOR, 'p[data-testid=\"promotion-compare-price\"]')\n",
        "                compare_price = compare_price_element.text.replace(\"Jmf pris \", \"\")\n",
        "            except NoSuchElementException:\n",
        "                try:\n",
        "                    compare_price_element = container.find_element(By.CSS_SELECTOR, 'p[data-testid=\"compare-price\"]')\n",
        "                    compare_price = compare_price_element.text.replace(\"Jmf pris \", \"\")\n",
        "                except NoSuchElementException:\n",
        "                    compare_price = np.nan  # Set to NaN if compare price is not found\n",
        "\n",
        "            # Try to retrieve the price from the current container\n",
        "            try:\n",
        "                price_element = container.find_element(By.CSS_SELECTOR, \"p[data-testid='price-text'] span[data-testid='price-container']\")\n",
        "                price = price_element.text\n",
        "            except NoSuchElementException:\n",
        "                price = np.nan  # Set to NaN if price is not found\n",
        "\n",
        "            # Save basic product information to the list\n",
        "            product_data.append({\n",
        "                \"CategoryLink\": category_link,\n",
        "                \"ProductLink\": product_link,\n",
        "                \"ProductName\": product_name,\n",
        "                \"ManufacturerName\": manufacturer_name,\n",
        "                \"Volume\": volume,\n",
        "                \"ComparePrice\": compare_price,\n",
        "                \"Price\": price,\n",
        "                \"Ingredients\": \"\"  # Empty for now, will fill later\n",
        "            })\n",
        "\n",
        "        except StaleElementReferenceException:\n",
        "          if debug:\n",
        "            # Debug print statement to log stale element exception when processing a product container\n",
        "            print(\"Stale element reference exception caught while extracting product data. Skipping this container.\")\n",
        "\n",
        "          continue\n",
        "\n",
        "    return product_data\n",
        "\n",
        "\"\"\"\n",
        "Define scrape_all_categories function.\n",
        "\n",
        "Main Functionality: Main function that orchestrates scraping of product data (name, manufacturer, volume, prices, etc.) from multiple categories.\n",
        "\"\"\"\n",
        "def scrape_all_categories(save_path='/content/drive/My Drive/scraped_files/', debug=False):\n",
        "    # Initialize a WebDriver instance with Chrome options\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # Initialize an empty list to store all scraped product data\n",
        "    all_product_data = []\n",
        "\n",
        "    # Define a list of excluded categories to skip during scraping\n",
        "    excluded_categories = [\n",
        "        \"https://www.hemkop.se/sortiment/hem-och-hushall\",\n",
        "        \"https://www.hemkop.se/sortiment/blommor-och-tillbehor\",\n",
        "        \"https://www.hemkop.se/sortiment/halsa-och-skonhet\",\n",
        "        \"https://www.hemkop.se/sortiment/apotek-och-lakemedel\",\n",
        "        \"https://www.hemkop.se/sortiment/djur\",\n",
        "        \"https://www.hemkop.se/sortiment/kiosk\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Retrieve main category links from the main page\n",
        "        main_category_links = get_category_links(driver, 'https://www.hemkop.se/handla')\n",
        "\n",
        "        # Filter out excluded categories from the main category links\n",
        "        filtered_category_links = [link for link in main_category_links if link not in excluded_categories]\n",
        "\n",
        "        # Iterate over each filtered category link to scrape product data\n",
        "        for category_link in filtered_category_links:\n",
        "\n",
        "            # Debug print statement to log the category being processed\n",
        "            if debug:\n",
        "              print(\"Processing category:\", category_link)\n",
        "\n",
        "            # Retrieve product data for the current category\n",
        "            product_df  = scrape_category_products(driver, category_link)\n",
        "\n",
        "            # Save the product data to a CSV file for the current category\n",
        "            if not product_df.empty:\n",
        "              # Extract the category name from the URL\n",
        "              category_name = category_link.split(\"/sortiment/\")[-1]\n",
        "\n",
        "              # Get today's date\n",
        "              today_date = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
        "\n",
        "              # Define the file name for the CSV file\n",
        "              csv_file_name = f\"{category_name}_{today_date}.csv\"\n",
        "\n",
        "               # Save locally in Colab\n",
        "              local_save_path = os.path.join('/content/', csv_file_name)\n",
        "              product_df.to_csv(local_save_path, index=False, encoding='utf-8')\n",
        "\n",
        "              # Debug print statement\n",
        "              if debug:\n",
        "                print(f\"Saved {category_name} data locally to {local_save_path}\")\n",
        "\n",
        "              # Save in Google Drive\n",
        "              drive_save_path = os.path.join(save_path, csv_file_name)\n",
        "              product_df.to_csv(drive_save_path, index=False, encoding='utf-8')\n",
        "\n",
        "              # Debug print statement\n",
        "              if debug:\n",
        "                print(f\"Saved {category_name} data to Google Drive: {drive_save_path}\")\n",
        "\n",
        "              # Append the product_df to all_product_data list\n",
        "              all_product_data.append(product_df)\n",
        "\n",
        "        # Concatenate all DataFrames in all_product_data into one DataFrame\n",
        "        if all_product_data:\n",
        "            product_df = pd.concat(all_product_data, ignore_index=True)\n",
        "            return product_df\n",
        "        else:\n",
        "          # Debug print statement if no data were scraped\n",
        "          if debug:\n",
        "            print(\"No product data scraped.\")\n",
        "\n",
        "          return pd.DataFrame([])\n",
        "\n",
        "    finally:\n",
        "        # Ensure WebDriver instance is properly closed after scraping\n",
        "        driver.quit()\n",
        "\n",
        "\"\"\"\n",
        "Define scrape_category_products function.\n",
        "\n",
        "Main Functionality: Opens a category page, retrieves total number of products, including scrolling to load more products,\n",
        "                    and scrapes product data until all products are retrieved.\n",
        "\"\"\"\n",
        "def scrape_category_products(driver, category_link, debug=False):\n",
        "    try:\n",
        "        # Initialize set to keep track of seen product links for this category\n",
        "        viewed_product_links = set()\n",
        "\n",
        "        # Initialize an empty list to store product data\n",
        "        product_data = []\n",
        "\n",
        "        # Navigate to the category page\n",
        "        driver.get(category_link)\n",
        "\n",
        "        # Debug print statement to log which category that was opened\n",
        "        if debug:\n",
        "          print(f\"Category opened: {category_link}\")\n",
        "\n",
        "        # Add small delay\n",
        "        time.sleep(30)\n",
        "\n",
        "        # Wait for the product containers to be loaded\n",
        "        WebDriverWait(driver, 25).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-testid=\"product-container\"]')))\n",
        "\n",
        "        # Get the total number of products\n",
        "        total_products_element = driver.find_element(By.CSS_SELECTOR, 'p.sc-85dd906a-0.jBHeLY')\n",
        "        total_products = int(total_products_element.text.split()[0])\n",
        "        if debug:\n",
        "          print(f\"Total number of products found: {total_products}\")\n",
        "\n",
        "        # Retrieve initial product containers\n",
        "        product_containers = scrape_visible_products(driver)\n",
        "        initial_product_data = scrape_product_data_from_containers(product_containers, category_link, viewed_product_links)\n",
        "        product_data.extend(initial_product_data)\n",
        "\n",
        "        # Debug print statement to log how many initial products that were scraped\n",
        "        if debug:\n",
        "          print(f\"Scraped {len(viewed_product_links)} out of {total_products} products initially.\")\n",
        "\n",
        "        # Continue scrolling and scraping until all products are retrieved from the category\n",
        "        while len(viewed_product_links) < total_products:\n",
        "            previous_seen_links_count = len(viewed_product_links)\n",
        "\n",
        "            # Scroll down to load more products\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "            # Debug print statement to log that more products are being loaded\n",
        "            if debug:\n",
        "              print(\"Scrolling to load more products...\")\n",
        "\n",
        "            # Wait for a short time to allow new products to load\n",
        "            time.sleep(30)\n",
        "\n",
        "            # Wait for new products to load\n",
        "            WebDriverWait(driver, 25).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-testid=\"product-container\"]')))\n",
        "\n",
        "            # Retrieve newly loaded product containers\n",
        "            new_product_containers = scrape_visible_products(driver)\n",
        "            new_product_data = scrape_product_data_from_containers(new_product_containers, category_link, viewed_product_links)\n",
        "            product_data.extend(new_product_data)\n",
        "\n",
        "            # Debug print statement to log how many products that have been scraped\n",
        "            if debug:\n",
        "              print(f\"Scraped {len(viewed_product_links)} out of {total_products} products.\")\n",
        "\n",
        "            if len(viewed_product_links) == previous_seen_links_count:\n",
        "\n",
        "                # Debug print statement to log that no new products will be loaded\n",
        "                if debug:\n",
        "                  print(\"No new products loaded. Breaking the loop.\")\n",
        "\n",
        "                break\n",
        "\n",
        "            # Break if we have scraped all products\n",
        "            if len(viewed_product_links) >= total_products:\n",
        "                break\n",
        "\n",
        "        # Convert product_data to DataFrame\n",
        "        product_df = pd.DataFrame(product_data)\n",
        "\n",
        "        return product_df\n",
        "\n",
        "    except TimeoutException as e:\n",
        "\n",
        "        # Debug print statement to log timeout error when loading products\n",
        "        if debug:\n",
        "          print(f\"Timeout waiting for products: {e}\")\n",
        "\n",
        "        return pd.DataFrame([])  # Return an empty DataFrame on timeout\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        # Debug print statement to log error when loading products\n",
        "        if debug:\n",
        "          print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "        return pd.DataFrame([])  # Return an empty DataFrame on any other exception\n",
        "\n"
      ],
      "metadata": {
        "id": "ediwO_0_d1ga"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define main functions for scraping ingredients data\n",
        "Functions responsible for the main scraping process of ingredients data:\n",
        "1.   *navigate_and_attempt(driver, product_link)*: Navigates to a product page and attempts to click the \"Produktfakta\" button.\n",
        "2.   *get_ingredient_info(driver, product_link)*: Retrieves ingredients information from a product page.\n",
        "3.   *retry_missing_ingredients(product_df, threshold)*: Checks for products without ingredients and retries to fetch missing information.\n",
        "4.   *scrape_ingredients_data(product_df)*: Main function that orchestrates scraping of ingredients data for products."
      ],
      "metadata": {
        "id": "vACdQEdukBIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define navigate_and_attempt function.\n",
        "\n",
        "Main Functionality: Navigates to a product page and attempts to find and click the \"Produktfakta\" button\n",
        "                    to later on retrieve the ingredients data from the product page.\n",
        "\"\"\"\n",
        "def navigate_and_attempt(driver, product_link, debug=False):\n",
        "\n",
        "    # Debug print statement to log which product that is being processed\n",
        "    if debug:\n",
        "      print(\"Navigating to product page:\", product_link)\n",
        "\n",
        "    # Navigate to the product page\n",
        "    driver.get(product_link)\n",
        "\n",
        "    # Add small delay\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Handle potential cookie consent banner\n",
        "    handle_cookie_consent(driver)\n",
        "\n",
        "    try:\n",
        "        # Debug print statement to log that the produktfakta button is being located\n",
        "        if debug:\n",
        "          print(f\"Attempt to locate 'Produktfakta' button.\")\n",
        "\n",
        "        # Ensure the page is fully loaded\n",
        "        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "\n",
        "        # Scroll down to ensure the button is in the viewport\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight / 3);\")\n",
        "\n",
        "        # Attempt to locate and click the \"Produktfakta\" button\n",
        "        produktfakta_button = WebDriverWait(driver, 30).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//button[@data-testid='tab' and .//div[contains(text(), 'Produktfakta')]]\"))\n",
        "        )\n",
        "\n",
        "        # Scroll to ensure the button is in the viewport\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView();\", produktfakta_button)\n",
        "\n",
        "        # Click the button\n",
        "        produktfakta_button.click()\n",
        "\n",
        "        # Debug print statement to log that the produktfakta button has been clicked on\n",
        "        if debug:\n",
        "          print(\"'Produktfakta' button clicked.\")\n",
        "\n",
        "        return True  # Indicate success\n",
        "\n",
        "    except TimeoutException:\n",
        "\n",
        "        # Debug print statement to log timeout error when trying to locate the produktfakta button\n",
        "        if debug:\n",
        "          print(\"Timeout: 'Produktfakta' button not found\")\n",
        "\n",
        "        # Call the take_screenshot function to take a screenshot of the current error shown on web page\n",
        "        take_screenshot(driver, product_link)\n",
        "\n",
        "        # Check if a 403 error occurred\n",
        "        if \"403\" in str(driver.page_source):\n",
        "\n",
        "            # Debug print statement to log a 403 error being detected\n",
        "            if debug:\n",
        "              print(\"403 error detected\")\n",
        "\n",
        "            return False  # Indicate 403 error\n",
        "\n",
        "    return None  # Indicate other types of failures\n",
        "\n",
        "\"\"\"\n",
        "Define get_ingredient_info function.\n",
        "\n",
        "Main Functionality: Navigates to a product page and retrieves ingredient information.\n",
        "\"\"\"\n",
        "def get_ingredient_info(driver, product_link, debug=False):\n",
        "\n",
        "    try:\n",
        "        attempt_result = navigate_and_attempt(driver, product_link)\n",
        "        if attempt_result is False:\n",
        "\n",
        "\n",
        "            if debug:\n",
        "              print(\"Refreshing page due to 403 error\")\n",
        "\n",
        "            # Refresh driver\n",
        "            driver.refresh()\n",
        "\n",
        "            # Add a small delay to mimic human behavior\n",
        "            time.sleep(10)\n",
        "\n",
        "            # Call the handle cookie consent function to reject the cookie consent banner\n",
        "            handle_cookie_consent(driver)\n",
        "\n",
        "            # Call the navigate_and_attemo function to attempt to locate and click the \"Produktfakta\" button to retrieve ingredients data\n",
        "            attempt_result = navigate_and_attempt(driver, product_link)\n",
        "\n",
        "        if attempt_result:\n",
        "            try:\n",
        "                ingredient_text = WebDriverWait(driver, 20).until(\n",
        "                    EC.presence_of_element_located((By.XPATH, \"//p[contains(text(), 'Innehållsdeklaration')]\"))\n",
        "                )\n",
        "                innehallsdeklaration_text = driver.find_element(By.XPATH, \"//p[contains(text(), 'Innehållsdeklaration')]/following-sibling::p\").text\n",
        "\n",
        "                # Debug print statement to log that ingredients have been found for a specific product\n",
        "                if debug:\n",
        "                  print(\"Ingredients found for product:\", product_link)\n",
        "\n",
        "                return innehallsdeklaration_text\n",
        "\n",
        "            except TimeoutException:\n",
        "                # Debug print statement to log timeout error for a specific product\n",
        "                if debug:\n",
        "                  print(f\"Error: Couldn't find ingredient data on {product_link}\")\n",
        "\n",
        "                return np.nan\n",
        "        else:\n",
        "            # Debug print statement to log that produkfakta button could not be found for a specific product\n",
        "            if debug:\n",
        "              print(f\"Error: 'Produktfakta' button not found on {product_link}\")\n",
        "            # If no ingredients could be found, return NaN\n",
        "            return np.nan\n",
        "\n",
        "    except WebDriverException as e:\n",
        "\n",
        "        # Debug print statement to log WebDriverException for a specific product\n",
        "        if debug:\n",
        "          print(\"Error while processing product link:\", product_link)\n",
        "          print(\"Error details:\", e)\n",
        "\n",
        "        # If no ingredients could be found due to WebDriverException, return NaN\n",
        "        return np.nan\n",
        "\n",
        "\"\"\"\n",
        "Define retry_missing_ingredients function.\n",
        "\n",
        "Main Functionality: Retries scraping ingredient data for products with missing information.\n",
        "\"\"\"\n",
        "def retry_missing_ingredients(product_df, threshold, debug=False):\n",
        "\n",
        "    while True:\n",
        "        missing_ingredients = product_df[product_df[\"Ingredients\"].isna()]\n",
        "        count_missing = len(missing_ingredients)\n",
        "\n",
        "        # Debug print statement to log how many products that have missing ingredients data\n",
        "        if debug:\n",
        "          print(f\"Number of products with missing ingredients: {count_missing}\")\n",
        "\n",
        "        if count_missing <= threshold:\n",
        "\n",
        "            # Debug print statement to log when number of products with misisng ingredients are below the threshold\n",
        "            if debug:\n",
        "              print(f\"The number of missing ingredients is now {count_missing}, which is below the threshold of {threshold}.\")\n",
        "            break\n",
        "\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        try:\n",
        "            for index, product in missing_ingredients.iterrows():\n",
        "                product_link = product[\"ProductLink\"]\n",
        "                product_df.at[index, \"Ingredients\"] = get_ingredient_info(driver, product_link)\n",
        "        finally:\n",
        "            driver.quit()\n",
        "\n",
        "    return product_df\n",
        "\n",
        "\"\"\"\n",
        "Define scrape_ingredients_data function.\n",
        "\n",
        "Main Functionality: Scrapes ingredient information for each product in a DataFrame.\n",
        "\"\"\"\n",
        "def scrape_ingredients_data(product_df, debug=False):\n",
        "\n",
        "    # Initialize a WebDriver instance with Chrome options\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    try:\n",
        "        # Iterate over each row (product) in the product_data DataFrame\n",
        "        for index, row in product_df.iterrows():\n",
        "            product_link = row[\"ProductLink\"]\n",
        "            product_df.at[index, \"Ingredients\"] = get_ingredient_info(driver, product_link)\n",
        "\n",
        "        # Retry to fill missing ingredients if needed\n",
        "        if \"Ingredients\" in product_df.columns:\n",
        "            final_missing_count = product_df[\"Ingredients\"].isna().sum()\n",
        "\n",
        "            # Define threshold for products with missing ingredients data per category\n",
        "            threshold = 5\n",
        "\n",
        "            if final_missing_count > threshold:\n",
        "\n",
        "                # Debug print statement to log how many prodcts that are still missing ingredients\n",
        "                if debug:\n",
        "                  print(f\"Final count of products with missing ingredients: {final_missing_count}\")\n",
        "                  print(\"Retry to fill missing ingredients...\")\n",
        "\n",
        "                # Call the retry_missing_ingredients function to retrieve ingredients data for products where it is missing\n",
        "                product_df = retry_missing_ingredients(product_df, threshold)\n",
        "\n",
        "            else:\n",
        "                # Debug print statement to log that count of products missing ingredients data is below the threshold\n",
        "                if debug:\n",
        "                  print(\"Count of products missing ingredients data is below threshold.\")\n",
        "\n",
        "        return product_df\n",
        "\n",
        "    finally:\n",
        "        # Ensure WebDriver instance is properly closed after scraping\n",
        "        driver.quit()"
      ],
      "metadata": {
        "id": "RBEsWS8lb7-d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution of Web Scraping Process for Product Data"
      ],
      "metadata": {
        "id": "3FF1LEJzl4mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call scrape_product_data function to scrape product data\n",
        "product_df = scrape_all_categories()\n",
        "\n",
        "print(product_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "collapsed": true,
        "id": "2DJf9lBsl5ST",
        "outputId": "e466ae35-2857-41de-eca4-fd8a46318a76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(104, 'Connection reset by peer')': /session/32e6426a7b634f6918708862776c34f4\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c621b4c6f50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/32e6426a7b634f6918708862776c34f4\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c621b4c6350>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/32e6426a7b634f6918708862776c34f4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f8f54a08e709>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call scrape_product_data function to scrape product data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproduct_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_all_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-afc5abe77065>\u001b[0m in \u001b[0;36mscrape_all_categories\u001b[0;34m(save_path, debug)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Retrieve product data for the current category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mproduct_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mscrape_category_products\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# Save the product data to a CSV file for the current category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-afc5abe77065>\u001b[0m in \u001b[0;36mscrape_category_products\u001b[0;34m(driver, category_link, debug)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;31m# Retrieve newly loaded product containers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mnew_product_containers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_visible_products\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mnew_product_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_product_data_from_containers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_product_containers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviewed_product_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mproduct_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_product_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-afc5abe77065>\u001b[0m in \u001b[0;36mscrape_product_data_from_containers\u001b[0;34m(product_containers, category_link, viewed_product_links, debug)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Get product link element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mproduct_link_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p[data-testid=\"product-title\"] a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mproduct_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct_link_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36mget_attribute\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetAttribute_js\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0m_load_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         attribute_value = self.parent.execute_script(\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;34mf\"/* getAttribute */return ({getAttribute_js}).apply(null, arguments);\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3C_EXECUTE_SCRIPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"script\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"args\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconverted_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             return self.request_encode_body(\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_from_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"assert_same_host\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36mconnection_from_host\u001b[0;34m(self, host, port, scheme, pool_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mrequest_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"host\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_from_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     def connection_from_context(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36mconnection_from_context\u001b[0;34m(self, request_context)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_key_constructor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mURLSchemeUnknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mpool_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_key_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_from_pool_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36m_default_key_normalizer\u001b[0;34m(key_class, request_context)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# namedtuples can't have fields starting with '_'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"key_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Default to ``None`` for keys missing from the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution of Web Scraping Process for Ingredients Data"
      ],
      "metadata": {
        "id": "5KIiPBuqmJO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define process_and_update_ingredients_csv_files function.\n",
        "\n",
        "Main Functionality: Processes all CSV files in the current directory, scrapes ingredient information for each product, and saves the updated DataFrame both locally and to a specified Google Drive path.\n",
        "\"\"\"\n",
        "\n",
        "def process_and_update_ingredients_csv_files(save_path='/content/drive/My Drive/scraped_files/', debug=False):\n",
        "  # List all CSV files in the current directory\n",
        "  csv_files = [f for f in glob.glob(\"*.csv\") if \"_with_ingredients\" not in f]\n",
        "\n",
        "  # Check if there are any CSV files found\n",
        "  if not csv_files:\n",
        "      # Debug print statement to log if no CSV files were found\n",
        "      if debug:\n",
        "        print(\"No CSV files found in the current directory.\")\n",
        "  else:\n",
        "    # Call the get_ingredients_info function to scrape data for each category\n",
        "    try:\n",
        "        for category_csv in csv_files:\n",
        "\n",
        "          # Debug print statement to log which csv is currently being processed\n",
        "          if debug:\n",
        "            print(f\"Processing CSV file: {category_csv}\")\n",
        "\n",
        "          # Detect the delimiter\n",
        "          delimiter = detect_delimiter(category_csv)\n",
        "\n",
        "          # Debug print statement to log the identified delimiter\n",
        "          if debug:\n",
        "              print(f\"Detected delimiter for {category_csv}: {delimiter}\")\n",
        "\n",
        "          # Read product data for the current category\n",
        "          product_df = pd.read_csv(category_csv, encoding='utf-8', sep=delimiter)\n",
        "\n",
        "          # Scrape ingredient data for the current DataFrame\n",
        "          updated_product_df = scrape_ingredients_data(product_df)\n",
        "\n",
        "          # Get today's date\n",
        "          today_date = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
        "\n",
        "          # Extract the base filename (i.e. category name before the first underscore)\n",
        "          base_filename = os.path.basename(category_csv).split('_')[0]\n",
        "\n",
        "          # Define the file name for the CSV file\n",
        "          updated_csv_file = f\"{base_filename}_with_ingredients_{today_date}.csv\"\n",
        "\n",
        "          # Save the updated DataFrame to a new CSV file with ingredients information to Colab\n",
        "          updated_local_csv_file = updated_csv_file\n",
        "          updated_product_df.to_csv(updated_local_csv_file, index=False)\n",
        "\n",
        "          # Debug print statement to log where data is being saved\n",
        "          if debug:\n",
        "              print(f\"Updated product data with ingredients information saved to {updated_local_csv_file}\")\n",
        "\n",
        "          # Save the updated DataFrame to a new CSV file with ingredients information to Google Drive\n",
        "          updated_drive_csv_file = os.path.join(save_path, updated_csv_file)\n",
        "          updated_product_df.to_csv(updated_drive_csv_file, index=False)\n",
        "\n",
        "          if debug:\n",
        "              print(f\"Saved updated product data with ingredients information to {updated_drive_csv_file}\")\n",
        "\n",
        "    finally:\n",
        "        # Debug print statement to log that processing is complete\n",
        "        if debug:\n",
        "          print(\"All processing complete.\")\n",
        "\n",
        "# Call the function\n",
        "process_and_update_ingredients_csv_files(debug=False)"
      ],
      "metadata": {
        "id": "7C3qlzzlmGpf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up and configure OpenAI"
      ],
      "metadata": {
        "id": "m6BhH9yYst9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0\n",
        "import openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESFZhrdX5mdf",
        "outputId": "b8216533-9766-433a-a944-7901cec0bf03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define set_openai_key function.\n",
        "\n",
        "Main Functionality: Loads and sets the OpenAI API key from an environment file, ensuring the key is available for use in the OpenAI module.\n",
        "\"\"\"\n",
        "def set_openai_key(debug=False):\n",
        "\n",
        "    try:\n",
        "        # Path to .env file in Colab environment\n",
        "        env_path = '/content/openai_api_key.env'\n",
        "\n",
        "        # Load environment variables from .env file\n",
        "        load_dotenv(env_path)\n",
        "        # Debug print statement\n",
        "        if debug:\n",
        "          print(f\"Environment variables loaded successfully from {env_path} file\")\n",
        "\n",
        "        # Access environment variables\n",
        "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "        # Check if the API key was retrieved successfully\n",
        "        if openai_api_key is None:\n",
        "            raise ValueError(\"No API key found in environment variables\")\n",
        "\n",
        "        # Set the OpenAI API key as an environment variable\n",
        "        os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "        # Set the API key for the OpenAI module\n",
        "        openai.api_key = openai_api_key\n",
        "\n",
        "        if debug:\n",
        "          print(\"OpenAI API key set successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"An error occurred while setting the OpenAI API key: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "BYbc5AJLP2zI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define function for NOVA classification of data"
      ],
      "metadata": {
        "id": "vFSsLVOxsmA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define classify_nova_food function.\n",
        "\n",
        "Main Functionality: Classifies food items into one of the four NOVA groups based on their ingredients and processing level using an OpenAI model.\n",
        "\"\"\"\n",
        "# Define the function to classify food into NOVA groups\n",
        "def classify_nova_food(product_name, ingredients, debug=False):\n",
        "\n",
        "    # Do not run the function if ingredients are missing\n",
        "    if pd.isna(ingredients) or ingredients.strip() == '' or ingredients == 'nan':\n",
        "        return None, None\n",
        "\n",
        "    prompt = (\n",
        "        \"Classify the following food item according to the NOVA classification system and decide which of the 4 groups the food item should be categorized as. \"\n",
        "        \"Provide the classification on the first line in your response. The 4 groups are as follows: \"\n",
        "        \"Group 1 - Unprocessed or minimally processed foods \"\n",
        "        \"Group 2 - Processed culinary ingredients \"\n",
        "        \"Group 3 - Processed foods \"\n",
        "        \"Group 4 - Ultra-processed foods \"\n",
        "        \"Set the classification to Unknown if it is not possible to categorize the item. \"\n",
        "\n",
        "        \"The 4 groups in the NOVA Classification system are the following: \"\n",
        "        \"Group 1 - Unprocessed or minimally processed foods like fruit, vegetables, eggs, meat, milk, etc. \"\n",
        "        \"Group 2 - Foods processed in the kitchen with the aim of extending their shelf life. In practice, these are ingredients to be used in the kitchen such as fats, \"\n",
        "        \"aromatic herbs, etc. to be kept in jars or in the refrigerator to be able to use them later. \"\n",
        "        \"Group 3 - Processed foods: Foods made by adding sugar, oil, salt, or other Group 2 ingredients to Group 1 foods. These processes include canning, bottling, and non-alcoholic fermentation. \"\n",
        "        \"Examples include canned vegetables, salted meats, cheese, and, importantly, tofu, which involves processing soybeans into soy milk, coagulating it, and pressing the curds into blocks. \"\n",
        "        \"These foods are made up of a few ingredients and are typically recognizable as modified versions of whole foods.\"\n",
        "        \"Group 4 - Ultra-processed foods. They are the ones that use many ingredients including food additives that improve palatability, processed raw materials (hydrogenated fats, modified starches, etc.) \"\n",
        "        \"and ingredients that are rarely used in home cooking such as soy protein or mechanically separated meat. These foods are mainly of industrial origin and are characterized by a good pleasantness and the fact \"\n",
        "        \"that they can be stored for a long time. \"\n",
        "\n",
        "        \"When classifying, carefully consider both the type of product by looking at the product name, the ingredients and the extent of processing involved, including any additives, preservatives, \"\n",
        "        \"or industrial processes used, with a preference for foods with less processing, fewer preservatives, and minimal added sugars that support good metabolic health. \"\n",
        "\n",
        "        f\"Given the food item {product_name} with the following ingredients {ingredients} and carefully considering the type of product, its production process and ingredients, \"\n",
        "        \"identify the NOVA group and give a brief explanation of why the food item was placed in the specific category. \"\n",
        "\n",
        "        \"Provide the classification on the first line, followed by a brief description on the subsequent lines. \"\n",
        "        \"Please keep the description brief, you do not need to repeat the product name of the food item in the description. \"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Debug print statement to log the prompt\n",
        "        if debug:\n",
        "          print(f\"Sending prompt to OpenAI: {prompt}\")\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",  # Use the appropriate model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a food classification assistant who wants to support metabolic health.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,  # Adjust the token limit as needed\n",
        "            temperature=0  # Set to 0 for more deterministic output\n",
        "        )\n",
        "\n",
        "        if debug:\n",
        "          # Debug print to log the received response\n",
        "          print(f\"Response received: {response}\")\n",
        "\n",
        "        # Extract the text from the response\n",
        "        classification_description = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "        # Split the response text into lines\n",
        "        classification_lines = classification_description.split('\\n')\n",
        "\n",
        "        # The first line is the classification\n",
        "        classification = classification_lines[0].strip()\n",
        "\n",
        "        # The rest of the lines form the brief description\n",
        "        brief_description = ' '.join(classification_lines[1:]).strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        classification = f\"Error: {e}\"\n",
        "        brief_description = f\"Error: {e}\"\n",
        "\n",
        "        # Debug print statement to log error\n",
        "        if debug:\n",
        "          print(f\"Error occurred: {classification}\")\n",
        "\n",
        "    # Return the classification and the brief description\n",
        "    return classification, brief_description\n"
      ],
      "metadata": {
        "id": "MjUpW6_PQ9ON",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution of NOVA Classification"
      ],
      "metadata": {
        "id": "dsvRpNN4s0Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Define the process_and_classify_nova_csv_files function.\n",
        "\n",
        "Main Functionality: Processes CSV files to classify food items into NOVA groups, then saves the updated data both locally and to Google Drive.\n",
        "\"\"\"\n",
        "def process_and_classify_nova_csv_files(save_path='/content/drive/My Drive/scraped_files/', debug=False):\n",
        "  # Set the OpenAI API key by calling the set_openai_key function\n",
        "  set_openai_key(debug=debug)\n",
        "\n",
        "  # List all CSV files in the current directory with ingredients data\n",
        "  csv_files = [f for f in glob.glob(\"*.csv\") if \"_with_ingredients\" in f]\n",
        "\n",
        "  # Check if there are any CSV files found\n",
        "  if not csv_files:\n",
        "      # Debug print statement to log if no csv files were found\n",
        "      if debug:\n",
        "        print(\"No CSV files found in the current directory.\")\n",
        "\n",
        "  else:\n",
        "      all_classified_data = []  # List to store all classified DataFrames\n",
        "\n",
        "      try:\n",
        "          for category_csv in csv_files:\n",
        "              # Skip already classified files\n",
        "              if \"_classified_data\" in category_csv:\n",
        "                  if debug:\n",
        "                      print(f\"Skipping already classified file: {category_csv}\")\n",
        "                  continue\n",
        "\n",
        "              # Debug print statement to log which category is being processed\n",
        "              if debug:\n",
        "                  print(f\"Processing CSV file: {category_csv}\")\n",
        "\n",
        "              # Detect the delimiter\n",
        "              delimiter = detect_delimiter(category_csv)\n",
        "\n",
        "              # Debug print statement to log the identified delimiter\n",
        "              if debug:\n",
        "                  print(f\"Detected delimiter for {category_csv}: {delimiter}\")\n",
        "\n",
        "              # Read product data for the current category\n",
        "              grocery_data_df = pd.read_csv(category_csv, encoding='utf-8', sep=delimiter)\n",
        "\n",
        "              # Ensure Ingredients column is treated as strings, handle NaNs appropriately\n",
        "              grocery_data_df['Ingredients'] = grocery_data_df['Ingredients'].astype(str)\n",
        "\n",
        "              # Apply the classification function to each row in the DataFrame\n",
        "              grocery_data_df['NOVA Classification'], grocery_data_df['NOVA Classification Description'] = zip(\n",
        "                  *grocery_data_df.apply(lambda row: classify_nova_food(row['ProductName'], row['Ingredients']), axis=1)\n",
        "              )\n",
        "\n",
        "              # Get today's date\n",
        "              today_date = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
        "\n",
        "              # Extract the base filename (i.e. category name before the first underscore)\n",
        "              base_filename = os.path.basename(category_csv).split('_')[0]\n",
        "\n",
        "              # Define the file name for the updated CSV file with classified data\n",
        "              final_csv_file = f\"{base_filename}_classified_data_{today_date}.csv\"\n",
        "\n",
        "              # Save the final DataFrame with classified data to a new CSV file to Colab\n",
        "              final_local_csv_file = final_csv_file\n",
        "              grocery_data_df.to_csv(final_local_csv_file, index=False)\n",
        "\n",
        "              # Debug print statement to log where data is being saved\n",
        "              if debug:\n",
        "                  print(f\"Updated product data with NOVA classification saved to {final_local_csv_file}\")\n",
        "\n",
        "              # Save the final DataFrame with classified data to a new CSV file to Google Drive\n",
        "              final_drive_csv_file = os.path.join(save_path, final_csv_file)\n",
        "              grocery_data_df.to_csv(final_drive_csv_file, index=False)\n",
        "\n",
        "              # Debug print statement to log where data is being saved\n",
        "              if debug:\n",
        "                  print(f\"Saved updated product data with NOVA classification information to {final_drive_csv_file}\")\n",
        "\n",
        "              # Append the classified DataFrame to all_classified_data\n",
        "              all_classified_data.append(grocery_data_df)\n",
        "\n",
        "      finally:\n",
        "          # Concatenate all DataFrames in all_classified_data into one final DataFrame\n",
        "          if all_classified_data:\n",
        "              final_combined_df = pd.concat(all_classified_data, ignore_index=True)\n",
        "\n",
        "              # Define the file name for the final combined CSV file\n",
        "              final_combined_csv_file = f\"all_categories_classified_data_{today_date}.csv\"\n",
        "\n",
        "              # Save the final combined DataFrame to Colab\n",
        "              final_local_combined_csv_file = final_combined_csv_file\n",
        "              final_combined_df.to_csv(final_local_combined_csv_file, index=False)\n",
        "\n",
        "              # Debug print statement to log where final combined data is being saved (Colab)\n",
        "              if debug:\n",
        "                  print(f\"Saved final combined classified data to Colab: {final_local_combined_csv_file}\")\n",
        "\n",
        "              # Save the final combined DataFrame to Google Drive\n",
        "              final_drive_combined_csv_file = os.path.join(save_path, final_combined_csv_file)\n",
        "              final_combined_df.to_csv(final_drive_combined_csv_file, index=False)\n",
        "\n",
        "              # Debug print statement to log where final combined data is being saved (Drive)\n",
        "              if debug:\n",
        "                  print(f\"Saved final combined classified data to Google Drive: {final_drive_combined_csv_file}\")\n",
        "          else:\n",
        "              # Debug print statement if no data were processed\n",
        "              if debug:\n",
        "                  print(\"No data processed for classification.\")\n",
        "\n",
        "          # Debug print statement to log that the classification is complete\n",
        "          if debug:\n",
        "              print(\"All NOVA classification complete.\")\n",
        "\n",
        "# Call the function\n",
        "process_and_classify_nova_csv_files(debug=False)"
      ],
      "metadata": {
        "id": "D52z_-wTsUXA",
        "collapsed": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6roTvyYXnjuI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}